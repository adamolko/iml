time_gd_1 <- system.time(
coef_gd_1 <- gradient_descent(step_size = 0.0001, X = X, y = y)
)["elapsed"]
time_gd_2 <- system.time(
coef_gd_2 <- gradient_descent(step_size = 0.00001, X = X, y = y)
)["elapsed"]
mse_lm <- mse(coef_lm, beta_truth)
mse_gd_1 <- mse(coef_gd_1, beta_truth)
mse_gd_2 <- mse(coef_gd_2, beta_truth)
# save results in list (performance, time)
result_list[[sim_nr]] <- data.frame(mse_lm = mse_lm,
mse_gd_1 = mse_gd_1,
mse_gd_2 = mse_gd_2,
time_lm = time_lm,
time_gd_1 = time_gd_1,
time_gd_2 = time_gd_2)
}
library(ggplot2)
library(dplyr)
library(tidyr)
do.call("rbind", result_list) %>%
gather() %>%
mutate(what = ifelse(grepl("mse", key), "MSE", "Time"),
algorithm = gsub("(mse|time)\\_(.*)","\\2", key)) %>%
ggplot(aes(x = algorithm, y = value)) +
geom_boxplot() + theme_bw() +
facet_wrap(~ what, scales = "free")
## ----eval=TRUE---------------------------------------------------------------------
#' @param step_size the step_size in each iteration
#' @param X the feature input matrix X
#' @param y the outcome vector y
#' @param beta a starting value for the coefficients
#' @param eps a small constant measuring the changes in each update step.
#' Stop the algorithm if the estimated model parameters do not change
#' more than \code{eps}.
#' @return a set of optimal coefficients beta
gradient_descent <- function(step_size, X, y, beta = rep(0,ncol(X)),
eps = 1e-8){
change <- 1 # something larger eps
XtX <- crossprod(X)
Xty <- crossprod(X,y)
while(sum(abs(change)) > eps){
# Use standard gradient descent:
change <- + step_size * (Xty - XtX%*%beta)
print(change)
# update beta in the end
beta <- beta + change
}
return(beta)
}
## ----eval=TRUE, fig.height=4, warning=FALSE, message=FALSE-------------------------
n <- 10000
p <- 100
nr_sims <- 20
# define mse
mse <- function(x,y) mean((x-y)^2)
# create data (only once)
X <- matrix(rnorm(n*p), ncol=p)
beta_truth <- runif(p, -2, 2)
f_truth <- X%*%beta_truth
# create result object
result_list <- vector("list", nr_sims)
# make it all reproducible
set.seed(2020-4-6)
for(sim_nr in nr_sims)
{
# create response
y <- f_truth + rnorm(n, sd = 2)
time_lm <- system.time(
coef_lm <- coef(lm(y~-1+X))
)["elapsed"]
time_gd_1 <- system.time(
coef_gd_1 <- gradient_descent(step_size = 0.0001, X = X, y = y)
)["elapsed"]
time_gd_2 <- system.time(
coef_gd_2 <- gradient_descent(step_size = 0.00001, X = X, y = y)
)["elapsed"]
mse_lm <- mse(coef_lm, beta_truth)
mse_gd_1 <- mse(coef_gd_1, beta_truth)
mse_gd_2 <- mse(coef_gd_2, beta_truth)
# save results in list (performance, time)
result_list[[sim_nr]] <- data.frame(mse_lm = mse_lm,
mse_gd_1 = mse_gd_1,
mse_gd_2 = mse_gd_2,
time_lm = time_lm,
time_gd_1 = time_gd_1,
time_gd_2 = time_gd_2)
}
library(ggplot2)
library(dplyr)
library(tidyr)
do.call("rbind", result_list) %>%
gather() %>%
mutate(what = ifelse(grepl("mse", key), "MSE", "Time"),
algorithm = gsub("(mse|time)\\_(.*)","\\2", key)) %>%
ggplot(aes(x = algorithm, y = value)) +
geom_boxplot() + theme_bw() +
facet_wrap(~ what, scales = "free")
#data = {35,8,10,23,42}
data.1<-c(35,8,10,23,42)
data.1
summary(data.1)
mean(data.1)
sd(data.1)
small.size.dataset=c(91,49,76,112,97,42,70, 100, 8, 112, 95, 90, 78, 62, 56, 94, 65, 58, 109, 70, 109, 91, 71, 76, 68, 62, 134, 57, 83, 66)
hist(small.size.dataset)
hist(small.size.dataset, xlabel = "My data points")
hist(small.size.dataset, xlab = "My data points")
hist(small.size.dataset, xlab = "My data points",
main = "Histogram of my data")
hist(small.size.dataset, xlab = "My data points",
main = "Histogram of my data",
freq = FALSE)
hist(small.size.dataset, xlab = "My data points",
main = "Histogram of my data",
freq = FALSE,
col = "green")
hist(small.size.dataset, xlab = "My data points",
main = "Histogram of my data",
freq = FALSE,
col = "black")
density(small.size.dataset)
lines(density(small.size.dataset))
hist(small.size.dataset, xlab = "My data points",
main = "Histogram of my data",
freq = FALSE,
col = "red")
lines(density(small.size.dataset))
lines(density(small.size.dataset), lw = 5)
hist(small.size.dataset, xlab = "My data points",
main = "Histogram of my data",
freq = FALSE,
col = "red", breaks = 10)
lines(density(small.size.dataset), lw = 5)
#bivariate data
set.seed(2016)
test1scores <- round(rnorm(50,78,10))
test2scores <- round(rnorm(50,70,14))
test1scores
#bivariate data
set.seed=2016
test1scores <- round(rnorm(50,78,10))
test2scores <- round(rnorm(50,70,14))
test1scores
plot(test2scores~test1scores)
plot(test2scores~test1scores, main = "Test scores for two exams (50 students)",
xlab = "Test 1 scores", ylab = "Test 2 scores")
plot(test2scores~test1scores, main = "Test scores for two exams (50 students)",
xlab = "Test 1 scores", ylab = "Test 2 scores",
col = "blue")
co2
class(co2)
plot(co2, main = "Atmosphetic CO2 Concentration")
co2.lm <- lm(co2~time(co2))
(co2.lm <- lm(co2~time(co2)))
plot(co2, main = "Atmospheric CO2 Concentration with Fitted Line")
abline(co2.lm)
(co2.lm <- lm(co2~time(co2)))
plot(co2, main = "Atmospheric CO2 Concentration with Fitted Line")
abline(co2.lm)
abline(co2.lm, col = "red")
co2.residuals <- resid(co2.lm)
(co2.lm <- lm(co2~time(co2)))istogram of Residuals")
hist(co2.residuals, main = "Histogram of Residuals")
#plotting histogram with only few datapoints is foolish
qqnorm(co2.residuals)
qqline(co2.residuals)
qqline(co2.residuals, col = "red")
plot(co2.residuals~time(co2))
plot(co2.residuals~time(co2), xlim = c(1960, 1963),
main = "Zoomed residuals")
help(sleep)
plot(extra~group, data = sleep, main = "Extra Sleep in Gossett Data by Group")
attach(sleep)
extra.1 <- extra[group==1]
extra.2 <- extra[group==2]
#testing
t.test(extra.1, extra.2, paired = TRUE,
alternative = "two.sided")
install.packages("astsa")
library(astsa)
help(jj)
plot(jj, type = "o", main = "Johnson&Johnson quearterly earnings per share",
xlab = "Year", ylab = "Earnings")
plot(flu, main = "Monthly Pneumonia and Influenza Deaths in US",
ylab = "Number of Deaths per 10,000 people", xlab = "Months")
plot(globtemp, main = "Global mean land-ocean deviations from average temperature of 1951-1980",
ylab = "Temperature deviations", xlab = "Years")
plot(globtemp, type = "o", main = "Global mean land-ocean deviations from average temperature of 1951-1980",
ylab = "Temperature deviations", xlab = "Years")
plot(globtempl, type = "o", main = "Global mean land deviations from average temperature of 1951-1980",
ylab = "Temperature deviations", xlab = "Years")
plot(star, main = "The magnitude of a star taken at midnight for 600 consecutive days",
ylab = "Magnitude", xlab = "Days")
purely_random_process <- ts(rnorm(100))
print(purely_random_process)
(acf(purely_random_process, type = "covariance"))
acf(purely_random_process,
main = "Correlogram of a purely random process")
(acf(purely_random_process,
main = "Correlogram of a purely random process"))
for (i in 2:1000) {
x[i] <- x[i-1]+rnorm(1)
}
x <- NULL
x[1] <- 0
for (i in 2:1000) {
x[i] <- x[i-1]+rnorm(1)
}
print(x)
random_walk <- ts(x)
plot(random_walk, main = "Random walk", ylab =" ", xlab = "days",
col = "blue", lwd = 2)
acf(random_walk)
plot(diff(random_walk))
# Generate noise
noise=rnorm(10000)
# Introduce a variable
ma_2=NULL
for(i in 3:10000){
ma_2[i]=noise[i]+0.7*noise[i-1]+0.2*noise[i-2]
}
# Shift data to left by 2 units
moving_average_process=ma_2[3:10000]
# Put time series structure on a vanilla data
moving_average_process=ts(moving_average_process)
# Partition output graphics as a multi frame of 2 rows and 1 column
par(mfrow=c(2,1))
# plot the process and plot its ACF
plot(moving_average_process, main='A moving average process of order 2', ylab=' ', col='blue')
acf(moving_average_process, main='Correlogram of a moving average process of order 2')
par("mar")
# Partition output graphics as a multi frame of 2 rows and 1 column
par(mfrow=c(2,1))
par(mar = c(1,1,1,1))
# plot the process and plot its ACF
plot(moving_average_process, main='A moving average process of order 2', ylab=' ', col='blue')
acf(moving_average_process, main='Correlogram of a moving average process of order 2')
acf(moving_average_process, main='Correlogram of a moving average process of order 2', ylab = "ACF")
# Partition output graphics as a multi frame of 2 rows and 1 column
par(mfrow=c(2,1))
par(mar = c(1,1,1,1))
# plot the process and plot its ACF
plot(moving_average_process, main='A moving average process of order 2', ylab=' ', col='blue')
acf(moving_average_process, main='Correlogram of a moving average process of order 2', ylab = "ACF")
# Partition output graphics as a multi frame of 2 rows and 1 column
par(mfrow=c(2,1))
par(mar = c(1,2,1,2))
# plot the process and plot its ACF
plot(moving_average_process, main='A moving average process of order 2', ylab=' ', col='blue')
acf(moving_average_process, main='Correlogram of a moving average process of order 2', ylab = "ACF")
x <- NULL
x[1] <- 0
for (i in 2:1000) {
x[i] <- x[i-1]+rnorm(1)
}
print(x)
random_walk <- ts(x)
plot(random_walk, main = "Random walk", ylab =" ", xlab = "days",
col = "blue", lwd = 2)
acf(random_walk)
#high correlation
plot(diff(random_walk))
#purely random process
set.seed(2016)
N <- 1000
set.seed(2016)
N <- 1000
phi <- 4
Z <- rnorm(N, 0, 1)
X <- NULL
X[1] <- Z[1]
for (t in 2:N) {
X[t] <- Z[t]+phi*X[t-1]
}
X.ts <- rs(X)
X.ts <- ts(X)
par(mfrow = c(2,1))
plot(X.ts, main = "AR(1) Time Series on White Noise, phi=0.4")
par(mar = c(1,1), mfrow = c(2,1))
par(mar = c(1,1,1,1), mfrow = c(2,1))
plot(X.ts, main = "AR(1) Time Series on White Noise, phi=0.4")
X.acf <- acf(X.ts, main = "AR(1) Time Series on White Noise, phi = 0.4")
X.ts <- ts(X)
par(mar = c(1,1,1,1), mfrow = c(2,1))
plot(X.ts, main = "AR(1) Time Series on White Noise, phi=0.4")
X.acf <- acf(X.ts, main = "AR(1) Time Series on White Noise, phi = 0.4")
isna(X.ts)
X.ts.isna()
X.ts
set.seed(2016)
N <- 1000
phi <- 0.4
Z <- rnorm(N, 0, 1)
X <- NULL
X[1] <- Z[1]
for (t in 2:N) {
X[t] <- Z[t]+phi*X[t-1]
}
X.ts <- ts(X)
par(mar = c(1,1,1,1), mfrow = c(2,1))
plot(X.ts, main = "AR(1) Time Series on White Noise, phi=0.4")
X.acf <- acf(X.ts, main = "AR(1) Time Series on White Noise, phi = 0.4")
#AR(2) process
set.seed(2017)
X.ts <- arima.sim(list(ar = c(0.7, 0.2)), n = 1000)
par(mfrow=c(2,1))
plot(X.ts, main = "AR(2) Time Series, phi1 = 0.7, phi2 = 0.2")
X.acf <- acf(X.ts, main = "Autocorrelaion of AR(2) Time Series")
#AR(2) with different phi's
phi1 <- 0.5
X.ts <- arima.sim(list(ar = c(phi1, phi2)), n=1000)
phi2 <- 0.4
X.ts <- arima.sim(list(ar = c(phi1, phi2)), n=1000)
par(mfrow = c(2,1))
plot(X.ts, main = paste("AR(2) Time Series, phi1=",phi1,"phi2=", phi2))
X.acf <- acf(X.ts, main = "Autocorrelation of AR(2) Time Series")
install.packages("isdals")
library(isdals)
data(bodyfat)
attach(bodyfat)
pairs(cbind(Fat, Triceps, Thigh, Midarm))
#thigh and triceps are good measures for body fat, but they are
#strongly correlated themselves
cor(cbind(Fat, Triceps, Thigh, Midarm))
Fat.hat <- predict(lm(Fat~Thigh))
Triceps.hat <- predict(lm(Triceps~Thigh))
cor((Fat~Fat.hat),(Triceps~Triceps.hat))
cor((Fat-Fat.hat),(Triceps-Triceps.hat))
install.packages("ppcor")
library(ppcor)
pcor(cbind(Fat, Triceps, Thigh))
#regressing on thigh and midarm
Fat.hat <- predict(lm(Fat~Thigh+Midarm))
Triceps.hat <- predict(lm(Triceps~Thigh+Midarm))
cor((Fat-Fat.hat), (Triceps-Triceps.hat))
pcor(cbind(Fat, Triceps, Thigh, Midarm))
sigma = 4
sigma <- 4
phi[1:2] <- c(1/3,1/2)
phi <- c(1/3,1/2)
n <- 10000
set.seed(2017)
ar.process <- arima.sim(n, model = list(ar = phi), sd = 4)
ar.process[1:5]
r <- acf(ar.process, plot = F)$acf[2:3]
R <- matrix(1,2,2)
R[1,2] <- r[1]
R[2,1] <- r[1]
R
b <- matrix(r,2,1)
b
solve(R,b)
phi.hat <- matrix(c(solve(R,b)[1.1], solve(R,b)[2,1],2,1)
phi.hat <- matrix(c(solve(R,b)[1.1], solve(R,b)[2,1]),2,1)
phi.hat <- matrix(c(solve(R,b)[1,1], solve(R,b)[2,1]),2,1)
phi.hat
c0 <- acf(ar.process, type = 'covariance', plot = F)$acf[1]
var.hat <- c0*(1-sum(phi.hat*r))
par(mfrow = c(3,1))
plot(ar.process, main = 'Simulated AR(2)')
acf(ar.process, main = "ACF")
par(mar = c(1,1,1,1))
par(mfrow = c(3,1))
plot(ar.process, main = 'Simulated AR(2)')
acf(ar.process, main = "ACF")
pacf(ar.process, main = 'PACF')
nr_points = 1000
p = 0.5
n = 100
# create data
X <- rbinom(nr_points, prob = p, size = n)
# define different Normal density functions
normal_optimal <- function(x) dnorm(x, mean = n*p, sd = sqrt(n*p*(1-p)))
normal_shift <- function(x) dnorm(x, mean = n*p - 10, sd = sqrt(n*p*(1-p)))
normal_scale_increase <- function(x) dnorm(x, mean = n*p, sd = sqrt(n*p*(1-p))*2)
normal_right_scale_decrease <- function(x) dnorm(x, mean = n*p + 20, sd = p*(1-p))
hist(X, breaks = 25, xlim = c(10, 100), freq = FALSE)
curve(normal_optimal, from = 10, to = 100, add = TRUE, col = "green")
curve(normal_shift, from = 10, to = 100, add = TRUE, col = "blue")
curve(normal_scale_increase, from = 10, to = 100, add = TRUE, col = "orange")
curve(normal_right_scale_decrease, from = 10, to = 100, add = TRUE, col = "red")
kld_value <- function(mu,sigma2)
{
0.5*log(sigma2) +
0.5 * (sigma2)^(-1) * (n*p*(1-p) + (n*p - mu)^2)
}
(optimal_green <- kld_value(n*p,n*p*(1-p)))
(shift_blue <- kld_value(n*p-10,n*p*(1-p)))
(scale_increase_orange <- kld_value(n*p,n*p*(1-p)*4))
(right_scale_decrease_red <- kld_value(n*p+20, (p*(1-p))^2))
#finding true values using a large sample from the true underlying distribution
p_seq <- seq(0.01, 0.99, l = 100)
n_seq <- seq(10, 500, by = 100)
B <- 10000
kld_value_approx <- function(n,p){
# sample a large number of data points from true distribution
x <- rbinom(B, prob = p, size = n)
# approximate the mean; threshold values to 0 if < 0 due
# to the approximation
pmax(
mean(
dbinom(x, prob = p, size = n, log = TRUE) -
dnorm(x, mean = n*p, sd = sqrt(n*p*(1-p)), log = TRUE),
na.rm = TRUE
),
0)
}
kld_val <- sapply(n_seq, function(this_n)
sapply(p_seq, function(this_p) kld_value_approx(this_n, this_p)))
cols = rev(colorRampPalette(c('darkred','red','blue','lightblue'))(50))
filled.contour(x = p_seq, y = n_seq, z = kld_val,
xlab = "p", ylab = "n",
col = cols
)
## ---- boosting_l_2
library(BBmisc)
library(rpart)
## ---- boosting_l_2
library(BBmisc)
library(rpart)
anim = function(X, y, M, minsplit = 1L) {
n = length(y)
x1 = X[,1L]
# for L2 loss we take the mean as constant model f_0
intercept = mean(y)
# here store the M models and associated beta weights
models = vector("list", M)
betas = numeric(M)
# prediction of first m additive models on data X
additivePredict = function(m) {
# if we had 0 models in the past, just return intercept
if (m == 0)
return(rep(intercept, n))
# otherwise predict first m model, multiply with beta, add intercept f_0
p = sapply(1:m, function(i) {
predict(models[[i]], newdata = X)
})
p %*% betas[1:m] + intercept
}
# loss = 0.5 (yhat - y)^2
# negative gradient of loss, here for L2-loss
dloss = function(yhat) (y - yhat)
# plot data and prediction yhat of first m models
plotModel = function(m, yhat, r, rhat, beta) {
par(mfrow = c(2L, 1L))
plot(x1, y, main = sprintf("data and first %i aditive models", m))
lines(x1, yhat)
plot(x1, r, main =
sprintf("pseudo-residuals r and rhat-fit of current model\nAfterwards we will find beta = %g", beta))
lines(x1, rhat)
par(mfrow = c(1L, 1L))
}
# get beta by line search
# (bonus points to reader who sees that this is actually unnessary and stupid for L2 loss...)
lineSearch = function(yhat, rhat) {
# L2 loss for yhat + beta * f.m
obj = function(beta)
crossprod(y - (yhat + beta * rhat))
# find best beta
or = optimize(obj, interval = c(0, 10000))
print(or)
or$minimum
}
for(j in 1:M) {
messagef("Iteration: %i", j)
# get predictions of our additive model
yhat = additivePredict(j - 1L)
# now get pseudo-residuals / negative gradient
r = dloss(yhat)
# fit model to pseudo residuals
rdata = cbind(X, r = r)
model = rpart(r ~ ., data = rdata, maxdepth = 1L, minsplit = 1L, minbucket = 1L, xval = 0L)
models[[j]] = model
rhat = predict(model, newdata = X)
trace = data.frame(x1 = x1, y = y, yhat = yhat, r = r, rhat = rhat)
print(trace)
beta = lineSearch(yhat = yhat, rhat = rhat)
plotModel(m = j - 1, yhat = yhat, r = r, rhat = rhat, beta = beta)
betas[[j]] = beta
pause()
}
return(list(models = models, betas = betas, intercept = intercept))
}
n = 10
x = seq(0, 10, length.out = n)
X = data.frame(x1 = x)
y = sin(x) + rnorm(n, mean = 0, sd = 0.01)
z = anim(X = X, y = y, M = 200)
data_daniel <- readRDS("~/Desktop/data_daniel.RDS")
print(data_daniel)
head(data_daniel)
data_daniel$music.artist
unique(data_daniel$music.artist)
columns(data_daniel)
data_daniel.colnames
colnames(data_daniel)
unique(data_daniel$id)
library(SHAPforxgboost)
library(xgboost)
library(tidyverse)
library(mlr)
library(SHAPforxgboost)
install.packages('r-shapforxgboost')
install.packages('shapforxgboost')
install.packages('SHAPforxgboost')
install.packages("remotes")
install_github('baddstats/polyclip')
install.packages("devtools")
install_github('baddstats/polyclip')
devtools::install_github('baddstats/polyclip')
setwd("~/Desktop/current research/iml")
