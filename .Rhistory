xlab = "Year", ylab = "Earnings")
plot(flu, main = "Monthly Pneumonia and Influenza Deaths in US",
ylab = "Number of Deaths per 10,000 people", xlab = "Months")
plot(globtemp, main = "Global mean land-ocean deviations from average temperature of 1951-1980",
ylab = "Temperature deviations", xlab = "Years")
plot(globtemp, type = "o", main = "Global mean land-ocean deviations from average temperature of 1951-1980",
ylab = "Temperature deviations", xlab = "Years")
plot(globtempl, type = "o", main = "Global mean land deviations from average temperature of 1951-1980",
ylab = "Temperature deviations", xlab = "Years")
plot(star, main = "The magnitude of a star taken at midnight for 600 consecutive days",
ylab = "Magnitude", xlab = "Days")
purely_random_process <- ts(rnorm(100))
print(purely_random_process)
(acf(purely_random_process, type = "covariance"))
acf(purely_random_process,
main = "Correlogram of a purely random process")
(acf(purely_random_process,
main = "Correlogram of a purely random process"))
for (i in 2:1000) {
x[i] <- x[i-1]+rnorm(1)
}
x <- NULL
x[1] <- 0
for (i in 2:1000) {
x[i] <- x[i-1]+rnorm(1)
}
print(x)
random_walk <- ts(x)
plot(random_walk, main = "Random walk", ylab =" ", xlab = "days",
col = "blue", lwd = 2)
acf(random_walk)
plot(diff(random_walk))
# Generate noise
noise=rnorm(10000)
# Introduce a variable
ma_2=NULL
for(i in 3:10000){
ma_2[i]=noise[i]+0.7*noise[i-1]+0.2*noise[i-2]
}
# Shift data to left by 2 units
moving_average_process=ma_2[3:10000]
# Put time series structure on a vanilla data
moving_average_process=ts(moving_average_process)
# Partition output graphics as a multi frame of 2 rows and 1 column
par(mfrow=c(2,1))
# plot the process and plot its ACF
plot(moving_average_process, main='A moving average process of order 2', ylab=' ', col='blue')
acf(moving_average_process, main='Correlogram of a moving average process of order 2')
par("mar")
# Partition output graphics as a multi frame of 2 rows and 1 column
par(mfrow=c(2,1))
par(mar = c(1,1,1,1))
# plot the process and plot its ACF
plot(moving_average_process, main='A moving average process of order 2', ylab=' ', col='blue')
acf(moving_average_process, main='Correlogram of a moving average process of order 2')
acf(moving_average_process, main='Correlogram of a moving average process of order 2', ylab = "ACF")
# Partition output graphics as a multi frame of 2 rows and 1 column
par(mfrow=c(2,1))
par(mar = c(1,1,1,1))
# plot the process and plot its ACF
plot(moving_average_process, main='A moving average process of order 2', ylab=' ', col='blue')
acf(moving_average_process, main='Correlogram of a moving average process of order 2', ylab = "ACF")
# Partition output graphics as a multi frame of 2 rows and 1 column
par(mfrow=c(2,1))
par(mar = c(1,2,1,2))
# plot the process and plot its ACF
plot(moving_average_process, main='A moving average process of order 2', ylab=' ', col='blue')
acf(moving_average_process, main='Correlogram of a moving average process of order 2', ylab = "ACF")
x <- NULL
x[1] <- 0
for (i in 2:1000) {
x[i] <- x[i-1]+rnorm(1)
}
print(x)
random_walk <- ts(x)
plot(random_walk, main = "Random walk", ylab =" ", xlab = "days",
col = "blue", lwd = 2)
acf(random_walk)
#high correlation
plot(diff(random_walk))
#purely random process
set.seed(2016)
N <- 1000
set.seed(2016)
N <- 1000
phi <- 4
Z <- rnorm(N, 0, 1)
X <- NULL
X[1] <- Z[1]
for (t in 2:N) {
X[t] <- Z[t]+phi*X[t-1]
}
X.ts <- rs(X)
X.ts <- ts(X)
par(mfrow = c(2,1))
plot(X.ts, main = "AR(1) Time Series on White Noise, phi=0.4")
par(mar = c(1,1), mfrow = c(2,1))
par(mar = c(1,1,1,1), mfrow = c(2,1))
plot(X.ts, main = "AR(1) Time Series on White Noise, phi=0.4")
X.acf <- acf(X.ts, main = "AR(1) Time Series on White Noise, phi = 0.4")
X.ts <- ts(X)
par(mar = c(1,1,1,1), mfrow = c(2,1))
plot(X.ts, main = "AR(1) Time Series on White Noise, phi=0.4")
X.acf <- acf(X.ts, main = "AR(1) Time Series on White Noise, phi = 0.4")
isna(X.ts)
X.ts.isna()
X.ts
set.seed(2016)
N <- 1000
phi <- 0.4
Z <- rnorm(N, 0, 1)
X <- NULL
X[1] <- Z[1]
for (t in 2:N) {
X[t] <- Z[t]+phi*X[t-1]
}
X.ts <- ts(X)
par(mar = c(1,1,1,1), mfrow = c(2,1))
plot(X.ts, main = "AR(1) Time Series on White Noise, phi=0.4")
X.acf <- acf(X.ts, main = "AR(1) Time Series on White Noise, phi = 0.4")
#AR(2) process
set.seed(2017)
X.ts <- arima.sim(list(ar = c(0.7, 0.2)), n = 1000)
par(mfrow=c(2,1))
plot(X.ts, main = "AR(2) Time Series, phi1 = 0.7, phi2 = 0.2")
X.acf <- acf(X.ts, main = "Autocorrelaion of AR(2) Time Series")
#AR(2) with different phi's
phi1 <- 0.5
X.ts <- arima.sim(list(ar = c(phi1, phi2)), n=1000)
phi2 <- 0.4
X.ts <- arima.sim(list(ar = c(phi1, phi2)), n=1000)
par(mfrow = c(2,1))
plot(X.ts, main = paste("AR(2) Time Series, phi1=",phi1,"phi2=", phi2))
X.acf <- acf(X.ts, main = "Autocorrelation of AR(2) Time Series")
install.packages("isdals")
library(isdals)
data(bodyfat)
attach(bodyfat)
pairs(cbind(Fat, Triceps, Thigh, Midarm))
#thigh and triceps are good measures for body fat, but they are
#strongly correlated themselves
cor(cbind(Fat, Triceps, Thigh, Midarm))
Fat.hat <- predict(lm(Fat~Thigh))
Triceps.hat <- predict(lm(Triceps~Thigh))
cor((Fat~Fat.hat),(Triceps~Triceps.hat))
cor((Fat-Fat.hat),(Triceps-Triceps.hat))
install.packages("ppcor")
library(ppcor)
pcor(cbind(Fat, Triceps, Thigh))
#regressing on thigh and midarm
Fat.hat <- predict(lm(Fat~Thigh+Midarm))
Triceps.hat <- predict(lm(Triceps~Thigh+Midarm))
cor((Fat-Fat.hat), (Triceps-Triceps.hat))
pcor(cbind(Fat, Triceps, Thigh, Midarm))
sigma = 4
sigma <- 4
phi[1:2] <- c(1/3,1/2)
phi <- c(1/3,1/2)
n <- 10000
set.seed(2017)
ar.process <- arima.sim(n, model = list(ar = phi), sd = 4)
ar.process[1:5]
r <- acf(ar.process, plot = F)$acf[2:3]
R <- matrix(1,2,2)
R[1,2] <- r[1]
R[2,1] <- r[1]
R
b <- matrix(r,2,1)
b
solve(R,b)
phi.hat <- matrix(c(solve(R,b)[1.1], solve(R,b)[2,1],2,1)
phi.hat <- matrix(c(solve(R,b)[1.1], solve(R,b)[2,1]),2,1)
phi.hat <- matrix(c(solve(R,b)[1,1], solve(R,b)[2,1]),2,1)
phi.hat
c0 <- acf(ar.process, type = 'covariance', plot = F)$acf[1]
var.hat <- c0*(1-sum(phi.hat*r))
par(mfrow = c(3,1))
plot(ar.process, main = 'Simulated AR(2)')
acf(ar.process, main = "ACF")
par(mar = c(1,1,1,1))
par(mfrow = c(3,1))
plot(ar.process, main = 'Simulated AR(2)')
acf(ar.process, main = "ACF")
pacf(ar.process, main = 'PACF')
nr_points = 1000
p = 0.5
n = 100
# create data
X <- rbinom(nr_points, prob = p, size = n)
# define different Normal density functions
normal_optimal <- function(x) dnorm(x, mean = n*p, sd = sqrt(n*p*(1-p)))
normal_shift <- function(x) dnorm(x, mean = n*p - 10, sd = sqrt(n*p*(1-p)))
normal_scale_increase <- function(x) dnorm(x, mean = n*p, sd = sqrt(n*p*(1-p))*2)
normal_right_scale_decrease <- function(x) dnorm(x, mean = n*p + 20, sd = p*(1-p))
hist(X, breaks = 25, xlim = c(10, 100), freq = FALSE)
curve(normal_optimal, from = 10, to = 100, add = TRUE, col = "green")
curve(normal_shift, from = 10, to = 100, add = TRUE, col = "blue")
curve(normal_scale_increase, from = 10, to = 100, add = TRUE, col = "orange")
curve(normal_right_scale_decrease, from = 10, to = 100, add = TRUE, col = "red")
kld_value <- function(mu,sigma2)
{
0.5*log(sigma2) +
0.5 * (sigma2)^(-1) * (n*p*(1-p) + (n*p - mu)^2)
}
(optimal_green <- kld_value(n*p,n*p*(1-p)))
(shift_blue <- kld_value(n*p-10,n*p*(1-p)))
(scale_increase_orange <- kld_value(n*p,n*p*(1-p)*4))
(right_scale_decrease_red <- kld_value(n*p+20, (p*(1-p))^2))
#finding true values using a large sample from the true underlying distribution
p_seq <- seq(0.01, 0.99, l = 100)
n_seq <- seq(10, 500, by = 100)
B <- 10000
kld_value_approx <- function(n,p){
# sample a large number of data points from true distribution
x <- rbinom(B, prob = p, size = n)
# approximate the mean; threshold values to 0 if < 0 due
# to the approximation
pmax(
mean(
dbinom(x, prob = p, size = n, log = TRUE) -
dnorm(x, mean = n*p, sd = sqrt(n*p*(1-p)), log = TRUE),
na.rm = TRUE
),
0)
}
kld_val <- sapply(n_seq, function(this_n)
sapply(p_seq, function(this_p) kld_value_approx(this_n, this_p)))
cols = rev(colorRampPalette(c('darkred','red','blue','lightblue'))(50))
filled.contour(x = p_seq, y = n_seq, z = kld_val,
xlab = "p", ylab = "n",
col = cols
)
## ---- boosting_l_2
library(BBmisc)
library(rpart)
## ---- boosting_l_2
library(BBmisc)
library(rpart)
anim = function(X, y, M, minsplit = 1L) {
n = length(y)
x1 = X[,1L]
# for L2 loss we take the mean as constant model f_0
intercept = mean(y)
# here store the M models and associated beta weights
models = vector("list", M)
betas = numeric(M)
# prediction of first m additive models on data X
additivePredict = function(m) {
# if we had 0 models in the past, just return intercept
if (m == 0)
return(rep(intercept, n))
# otherwise predict first m model, multiply with beta, add intercept f_0
p = sapply(1:m, function(i) {
predict(models[[i]], newdata = X)
})
p %*% betas[1:m] + intercept
}
# loss = 0.5 (yhat - y)^2
# negative gradient of loss, here for L2-loss
dloss = function(yhat) (y - yhat)
# plot data and prediction yhat of first m models
plotModel = function(m, yhat, r, rhat, beta) {
par(mfrow = c(2L, 1L))
plot(x1, y, main = sprintf("data and first %i aditive models", m))
lines(x1, yhat)
plot(x1, r, main =
sprintf("pseudo-residuals r and rhat-fit of current model\nAfterwards we will find beta = %g", beta))
lines(x1, rhat)
par(mfrow = c(1L, 1L))
}
# get beta by line search
# (bonus points to reader who sees that this is actually unnessary and stupid for L2 loss...)
lineSearch = function(yhat, rhat) {
# L2 loss for yhat + beta * f.m
obj = function(beta)
crossprod(y - (yhat + beta * rhat))
# find best beta
or = optimize(obj, interval = c(0, 10000))
print(or)
or$minimum
}
for(j in 1:M) {
messagef("Iteration: %i", j)
# get predictions of our additive model
yhat = additivePredict(j - 1L)
# now get pseudo-residuals / negative gradient
r = dloss(yhat)
# fit model to pseudo residuals
rdata = cbind(X, r = r)
model = rpart(r ~ ., data = rdata, maxdepth = 1L, minsplit = 1L, minbucket = 1L, xval = 0L)
models[[j]] = model
rhat = predict(model, newdata = X)
trace = data.frame(x1 = x1, y = y, yhat = yhat, r = r, rhat = rhat)
print(trace)
beta = lineSearch(yhat = yhat, rhat = rhat)
plotModel(m = j - 1, yhat = yhat, r = r, rhat = rhat, beta = beta)
betas[[j]] = beta
pause()
}
return(list(models = models, betas = betas, intercept = intercept))
}
n = 10
x = seq(0, 10, length.out = n)
X = data.frame(x1 = x)
y = sin(x) + rnorm(n, mean = 0, sd = 0.01)
z = anim(X = X, y = y, M = 200)
data_daniel <- readRDS("~/Desktop/data_daniel.RDS")
print(data_daniel)
head(data_daniel)
data_daniel$music.artist
unique(data_daniel$music.artist)
columns(data_daniel)
data_daniel.colnames
colnames(data_daniel)
unique(data_daniel$id)
library(SHAPforxgboost)
library(xgboost)
library(tidyverse)
library(mlr)
library(SHAPforxgboost)
install.packages('r-shapforxgboost')
install.packages('shapforxgboost')
install.packages('SHAPforxgboost')
install.packages("remotes")
install_github('baddstats/polyclip')
install.packages("devtools")
install_github('baddstats/polyclip')
devtools::install_github('baddstats/polyclip')
sessionInfo()
installed.packages()
tail(sessionInfo())
packageVersion("iml")
packageVersion("xgboost")
packageVersion("tidyverse")
packageVersion("mlr")
packageVersion("viridis")
packageVersion("featureImportance")
packageVersion("Hmisc")
packageVersion("caret")
packageVersion("Metrics")
packageVersion("plyr")
packageVersion("dplyr")
packageVersion("readr")
packageVersion("glmnet")
packageVersion("forcats")
packageVersion("corrplot")
packageVersion("cmaes")
packageVersion("mlrCPO")
packageVersion("ggplot2")
packageVersion("partykit")
packageVersion("variables")
packageVersion("rpart")
packageVersion("rpart.plot")
devtools::install_github("giuseppec/featureImportance")
packageVersion("featureImportance")
devtools::install_github("liuyanguu/SHAPforxgboost")
/mlr
?viridis
version("data.table")
packageVersion("data.table")
setwd("~/Desktop/current research/iml")
library(Hmisc)
library(caret)
library(Metrics)
library(readr)
library(plyr)
library(glmnet)
library(forcats)
library(corrplot)
library(rpart)
library(rpart.plot)
library(ipred)
library(mctest)
library(randomForest)
library(tidyverse)
library(mlr)
path_train  <- "data/train.csv"
cleaning_path_train <- "cleaning_train.R"
source(cleaning_path_train)
data = housing
#-------------------------
#LINEAR MODEL
set.seed(123)
# smp_size <- floor(0.75 * nrow(data))
# train_ind <- sample(seq_len(nrow(data)), size = smp_size)
# train_ind had the tendency to change even with the seed, so we saved them
# saveRDS(train_ind, "results/train_ind.rds")
train_ind <- readRDS("results/train_ind.rds")
# smp_size <- floor(0.75 * nrow(data))
# train_ind <- sample(seq_len(nrow(data)), size = smp_size)
# train_ind had the tendency to change even with the seed, so we saved them
# saveRDS(train_ind, "results/train_ind.rds")
train_ind <- readRDS("results/train_ind.rds")
#one-hot encoding
not_dummies <- c("LotArea", "YearBuilt", "TotalBsmtSF",
"GrLivArea", "porch_area", "SalePrice",
"FullBath", "HalfBath", "BedroomAbvGr",
"KitchenAbvGr", "GarageCars", "OverallQual",
"OverallCond", "CentralAir", "PavedDrive",
"pool", "Remod", "ExterQual", "ExterCond", "BsmtQual", "HeatingQC",  "KitchenQual", "BsmtFinType1", "FireplaceQu")
dummies <- select(data, -all_of(not_dummies))
dmy <- dummyVars(" ~ .", data = dummies)
trsf <- data.frame(predict(dmy, newdata = dummies))
for (name in not_dummies) {
trsf[name] <- data[name]
}
#removing one level for each dummy encoded categorical feature to avoid perfect multicollinearity
reference_levels <- c("MSZoning.RH", "LotShape.IR2.5",
"Neighborhood.Blueste", "BldgType.Twnhs",
"RoofStyle.Other", "Foundation.Other",
"Functional.Maj",
"SaleType.Other", "SaleCondition.Other",
"SeasonSold.w", "porch_type.no", "Electrical.SBrkr")
trsf <- select(trsf, -all_of(reference_levels))
#75/25 train/test split
train <- trsf[train_ind, ]
test <- trsf[-train_ind, ]
train_X <- select(train, -SalePrice)
train_y <- train$SalePrice
test_X <- select(test, -SalePrice)
test_y <- test$SalePrice
#linear model without interactions
model_lm <- lm(SalePrice~., data = train)
summary(model_lm)
predictions_lm <- predict(model_lm, test_X)
rmse_lm <- rmse(test_y, predictions_lm)
rmse_lm
rmsle_lm <- rmsle(test_y, predictions_lm)
rmsle_lm
#--------------------------------------
#LASSO
set.seed(123)
#choosing best lambda value based on training data
lambdas <- 10^seq(3, -3, by = -.1)
lasso_reg <- cv.glmnet(data.matrix(train_X), train_y, alpha = 1, lambda = lambdas, standardize = TRUE, nfolds = 5)
lambda_best <- lasso_reg$lambda.min
lambda_best
#actually fitting the model
lasso_model <- glmnet(data.matrix(train_X), train_y, alpha = 1, lambda = lambda_best, standardize = TRUE)
predictions_lasso <- predict(lasso_model, s = lambda_best, newx = data.matrix(test_X))
rmse_lasso <- rmse(test_y, predictions_lasso)
rmse_lasso
#choosing best lambda value based on training data
lambdas <- 10^seq(2, -3, by = -.1)
lasso_reg <- cv.glmnet(data.matrix(train_X), train_y, alpha = 1, lambda = lambdas, standardize = TRUE, nfolds = 5)
lambda_best <- lasso_reg$lambda.min
lambda_best
#actually fitting the model
lasso_model <- glmnet(data.matrix(train_X), train_y, alpha = 1, lambda = lambda_best, standardize = TRUE)
predictions_lasso <- predict(lasso_model, s = lambda_best, newx = data.matrix(test_X))
rmse_lasso <- rmse(test_y, predictions_lasso)
rmse_lasso
rmsle_lasso <- rmsle(test_y, predictions_lasso)
rmsle_lasso
#choosing best lambda value based on training data
lambdas <- 10^seq(3, -3, by = -.1)
lasso_reg <- cv.glmnet(data.matrix(train_X), train_y, alpha = 1, lambda = lambdas, standardize = TRUE, nfolds = 5)
lambda_best <- lasso_reg$lambda.min
lambda_best
#actually fitting the model
lasso_model <- glmnet(data.matrix(train_X), train_y, alpha = 1, lambda = lambda_best, standardize = TRUE)
predictions_lasso <- predict(lasso_model, s = lambda_best, newx = data.matrix(test_X))
rmse_lasso <- rmse(test_y, predictions_lasso)
rmse_lasso
rmsle_lasso <- rmsle(test_y, predictions_lasso)
rmsle_lasso
#--------------------------------------
#Random Forest
set.seed(123)
# train with the default parameters
random_forest<- randomForest(x = train_X, y=train_y)
pred_randomForest =  predict(random_forest, test_X)
rmse_randomForest <- rmse(test_y, pred_randomForest)
rmse_randomForest
rmsle_randomForest <- rmsle(test_y, pred_randomForest)
rmsle_randomForest
#Small hyperparameter tuning:
traintask <- makeRegrTask(data = train,target = "SalePrice")
testtask <- makeRegrTask(data = test, target = "SalePrice")
#Run best model with these parameters
set.seed(123)
parameters = mytune$x
mytune <- readRDS("results/tuning_result_randomForest.rds")
#Run best model with these parameters
set.seed(123)
parameters = mytune$x
lrn_tune <- setHyperPars(lrn,par.vals = parameters)
random_forest <- train(learner = lrn_tune,task = traintask)
pred_randomForest <- predict(random_forest,testtask)
rmse_randomForest <- rmse(pred_randomForest$data$truth, pred_randomForest$data$response)
rmse_randomForest
rmsle_randomForest <- rmsle(pred_randomForest$data$truth, pred_randomForest$data$response)
rmsle_randomForest
#Small hyperparameter tuning:
traintask <- makeRegrTask(data = train,target = "SalePrice")
testtask <- makeRegrTask(data = test, target = "SalePrice")
lrn <- makeLearner("regr.randomForest",predict.type = "response")
params <- makeParamSet( makeIntegerParam("maxnodes",lower = 1,upper = 300),
makeIntegerParam("nodesize",lower = 1,upper = 10),
makeIntegerParam("ntree",lower = 200,upper = 500))
rdesc <- makeResampleDesc(iters = 5, "CV")
ctrl <- makeTuneControlRandom(budget = 100)
#Run best model with these parameters
set.seed(123)
parameters = mytune$x
lrn_tune <- setHyperPars(lrn,par.vals = parameters)
random_forest <- train(learner = lrn_tune,task = traintask)
pred_randomForest <- predict(random_forest,testtask)
rmse_randomForest <- rmse(pred_randomForest$data$truth, pred_randomForest$data$response)
rmse_randomForest
rmsle_randomForest <- rmsle(pred_randomForest$data$truth, pred_randomForest$data$response)
rmsle_randomForest
